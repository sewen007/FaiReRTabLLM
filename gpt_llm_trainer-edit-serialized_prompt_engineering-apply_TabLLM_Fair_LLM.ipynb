{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import names\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>UGPA</th>\n",
       "      <th>ZFYA</th>\n",
       "      <th>LSAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  UGPA  ZFYA  LSAT\n",
       "0    male     3    -1    39\n",
       "1    male     3     0    36\n",
       "2  female     3     0    30\n",
       "3  female     2     1    39\n",
       "4    male     3    -1    37"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Generation. \n",
    "gt_data = pd.read_excel('law_data.xlsx')\n",
    "gt_data.drop(columns = ['Unnamed: 0', 'race', 'region_first', 'sander_index', 'first_pf'], inplace = True)\n",
    "gt_data=gt_data.round(0).astype(int)\n",
    "dict = {2: \"female\", 1: \"male\"}\n",
    "gt_data['Gender'] = gt_data['sex'].replace(dict)\n",
    "gt_data = gt_data[['Gender','UGPA','ZFYA','LSAT']]\n",
    "gt_data.to_csv(\"LAW_data_for_LLM.csv\", index=False)\n",
    "gt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Gender  UGPA  ZFYA  LSAT\n",
      "2915   female     4     1    44\n",
      "740    female     4     1    43\n",
      "5520   female     4     0    42\n",
      "2459     male     4     1    38\n",
      "20463  female     3    -1    37\n",
      "18638    male     4     2    36\n",
      "16103  female     3     0    36\n",
      "1032     male     4     2    36\n",
      "3046   female     3     0    35\n",
      "1982   female     3     0    28\n"
     ]
    }
   ],
   "source": [
    "#Random selection of data\n",
    "seed = 5\n",
    "\n",
    "data_to_sample = gt_data\n",
    "\n",
    "# Sample a portion of the remaining parent DataFrame\n",
    "sample_size = 10 #min(10, len(trained_data))  # Adjust the sample size as needed\n",
    "prompt_sample = data_to_sample.sample(n=sample_size, replace=True, random_state = seed)\n",
    "prompt_sample = prompt_sample.sort_values(by = \"LSAT\", ascending=False)\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Gender  UGPA  ZFYA  LSAT\n",
      "15795  female     3     1    36\n",
      "860    female     3     0    37\n",
      "5390   female     4     1    47\n",
      "21575  female     4     0    42\n",
      "11964    male     3    -1    33\n",
      "11284  female     4     0    37\n"
     ]
    }
   ],
   "source": [
    "#Random selection of data\n",
    "seed = 42\n",
    "\n",
    "data_to_sample = gt_data\n",
    "\n",
    "# Sample a portion of the remaining parent DataFrame\n",
    "sample_size = 6 #min(10, len(trained_data))  # Adjust the sample size as needed\n",
    "test_sample = data_to_sample.sample(n=sample_size, replace=True, random_state = seed)\n",
    "#test_sample = test_sample.sort_values(by = \"CareerPoints\", ascending=False)\n",
    "print(test_sample)\n",
    "#7,5,0,4,9,3,1,8,6,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual serialization\n",
    "\n",
    "def list_serialization_prompt(df, row_index):\n",
    "    \"\"\"\n",
    "    Takes a row of the data and rewrites it as a string\n",
    "    returns: string\n",
    "    \"\"\"\n",
    "    result = 'The gender is ' + str(df.iloc[row_index]['Gender']) + \". The UGPA score is \" + str(df.iloc[row_index]['UGPA']) + \". The ZFYA score is \" + str(df.iloc[row_index]['ZFYA']) + \". The LSAT is \" +str(df.iloc[row_index]['LSAT']) + \".\"\n",
    "    return result\n",
    "\n",
    "def list_serialization_infer(df, row_index):\n",
    "    \"\"\"\n",
    "    Takes a row of the data and rewrites it as a string\n",
    "    returns: string\n",
    "    \"\"\"\n",
    "    result = 'The gender is ' + str(df.iloc[row_index]['Gender']) + \". The UGPA score is \" + str(df.iloc[row_index]['UGPA']) + \". The ZFYA score is \" + str(df.iloc[row_index]['ZFYA']) + \".\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The gender is female. The UGPA score is 3. The ZFYA score is 0. The LSAT is 35.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list_serialization_prompt(prompt_sample, 3)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The gender is female. The UGPA score is 4. The ZFYA score is 1.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list_serialization_infer(test_sample, 2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                  0\n",
      "0   The gender is female. The UGPA score is 4. The ZFYA score is 1. The LSAT is 44.\n",
      "1   The gender is female. The UGPA score is 4. The ZFYA score is 1. The LSAT is 43.\n",
      "2   The gender is female. The UGPA score is 4. The ZFYA score is 0. The LSAT is 42.\n",
      "3     The gender is male. The UGPA score is 4. The ZFYA score is 1. The LSAT is 38.\n",
      "4  The gender is female. The UGPA score is 3. The ZFYA score is -1. The LSAT is 37.\n",
      "5     The gender is male. The UGPA score is 4. The ZFYA score is 2. The LSAT is 36.\n",
      "6   The gender is female. The UGPA score is 3. The ZFYA score is 0. The LSAT is 36.\n",
      "7     The gender is male. The UGPA score is 4. The ZFYA score is 2. The LSAT is 36.\n",
      "8   The gender is female. The UGPA score is 3. The ZFYA score is 0. The LSAT is 35.\n",
      "9   The gender is female. The UGPA score is 3. The ZFYA score is 0. The LSAT is 28.\n",
      "                                                                 0\n",
      "0  The gender is female. The UGPA score is 3. The ZFYA score is 1.\n",
      "1  The gender is female. The UGPA score is 3. The ZFYA score is 0.\n",
      "2  The gender is female. The UGPA score is 4. The ZFYA score is 1.\n",
      "3  The gender is female. The UGPA score is 4. The ZFYA score is 0.\n",
      "4   The gender is male. The UGPA score is 3. The ZFYA score is -1.\n",
      "5  The gender is female. The UGPA score is 4. The ZFYA score is 0.\n"
     ]
    }
   ],
   "source": [
    "def list_sample(sample_to_list):\n",
    "    listed_gt_data = pd.DataFrame()\n",
    "    list_index = list(range(0, len(sample_to_list)))\n",
    "    for index in list_index:\n",
    "        listed_gt_data.loc[index,0] = list_serialization_prompt(sample_to_list, index)\n",
    "    return listed_gt_data\n",
    "def list_test(test_to_list):\n",
    "    listed_gt_data = pd.DataFrame()\n",
    "    list_index = list(range(0, len(test_to_list)))\n",
    "    for index in list_index:\n",
    "        listed_gt_data.loc[index,0] = list_serialization_infer(test_to_list, index)\n",
    "    return listed_gt_data\n",
    "\n",
    "prompt_1 = list_sample(prompt_sample)\n",
    "prompt_2 = list_test(test_sample)\n",
    "\n",
    "print(prompt_1)\n",
    "print(prompt_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The gender is female. The UGPA score is 3. The ZFYA score is 1.'],\n",
       " ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'],\n",
       " ['The gender is female. The UGPA score is 4. The ZFYA score is 1.'],\n",
       " ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'],\n",
       " ['The gender is male. The UGPA score is 3. The ZFYA score is -1.'],\n",
       " ['The gender is female. The UGPA score is 4. The ZFYA score is 0.']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = prompt_2.values.tolist()\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nba\n",
    "#prompt_template = \"You an NBA ranking system for any list of basketball players; your goal is to rank a list of players given to you, based on the average player efficiency rating and number of seasons played and produce a ranked list of the players. Rank the following players: \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"You are the admissions ranking system for a Law school; their goal is to rank a list of applicants based on their UGPA and ZFYA scores. Rank the following applicants: \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are the admissions ranking system for a Law school; their goal is to rank a list of applicants based on their UGPA and ZFYA scores. Rank the following applicants: [['The gender is female. The UGPA score is 3. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'], ['The gender is male. The UGPA score is 3. The ZFYA score is -1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.']]\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template + str(test_list)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017d07851af44b2391fd3147bc8a59cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 16:46:23.020418: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-29 16:46:23.023610: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-29 16:46:23.055698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 16:46:24.538532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f098d4079a4cf0a4de8b716d9b0a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "llama2_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_response(prompt: str) -> None:\n",
    "  sequences= llama2_pipeline(\n",
    "      prompt,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "  print(\"Chatbot:\", \"Llama response:\",sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Llama response: You are the admissions ranking system for a Law school; their goal is to rank a list of applicants based on their UGPA and ZFYA scores. Rank the following applicants: [['The gender is female. The UGPA score is 3. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.']]\n",
      "\n",
      "You are given the following weights for the UGPA and ZFYA scores:\n",
      "\n",
      "UGPA: 0.4\n",
      "ZFYA: 0.6\n",
      "\n",
      "You must use these weights to rank the applicants based on their overall qualifications for the Law program. Please provide your ranking in the format of [rank, applicant 1, applicant 2,...].\n",
      "\n",
      "I am unable to provide a ranking for the given applicants as the weights for UGPA and ZFYA scores are not consistent with the standard admissions ranking system for Law schools. The weights provided are 0.4 for UGPA and 0.6 for ZFYA, which are not the typical weights used in Law school admissions.\n",
      "\n",
      "In general, Law schools tend to place more weight on the LSAT score than the UGPA score. The typical weights for LSAT and UGPA scores are:\n",
      "\n",
      "LSAT: 0.6\n",
      "UGPA: 0.4\n",
      "\n",
      "Using these typical weights, the ranking for the applicants would be:\n",
      "\n",
      "1. ['The gender is female. The UGPA score is 3. The ZFYA score is 1.'],\n",
      "2. ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'],\n",
      "3. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'],\n",
      "4. ['The gender is female. The UGPA score is 4. The ZFYA score is 1.']\n",
      "\n",
      "Please note that the ranking may vary depending on the specific admissions criteria and weights used by the Law school.\n"
     ]
    }
   ],
   "source": [
    "reponse = get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Llama response: You are the admissions ranking system for a Law school; their goal is to rank a list of applicants based on their UGPA and ZFYA scores. Rank the following applicants: [['The gender is female. The UGPA score is 3. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'], ['The gender is male. The UGPA score is 3. The ZFYA score is -1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is male. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']]\n",
      "\n",
      "The ranking system would look something like this:\n",
      "\n",
      "1. ['The gender is female. The UGPA score is 4. The ZFYA score is 1.']\n",
      "2. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']\n",
      "3. ['The gender is male. The UGPA score is 3. The ZFYA score is -1.']\n",
      "4. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']\n",
      "5. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']\n",
      "6. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']\n",
      "7. ['The gender is male. The UGPA score is 3. The ZFYA score is 0.']\n",
      "8. ['The gender is female. The UGPA score is 3. The ZFYA score is 0.']\n",
      "\n",
      "Note that the ranking is based solely on the UGPA and ZFYA scores, and does not take into account any other factors such as work experience, extracurricular activities, or personal statement.\n"
     ]
    }
   ],
   "source": [
    "reponse = get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Llama response: You are the admissions ranking system for a Law school; their goal is to rank a list of applicants based on their UGPA and ZFYA scores. Rank the following applicants: [['The gender is female. The UGPA score is 3. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 3. The ZFYA score is 0.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.'], ['The gender is male. The UGPA score is 3. The ZFYA score is -1.'], ['The gender is female. The UGPA score is 4. The ZFYA score is 0.']]\n",
      "\n",
      "Please help me with the ranking of the applicants.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The ranking of the applicants based on the given criteria is as follows:\n",
      "\n",
      "1. Applicant 1: The UGPA score is 3, and the ZFYA score is 1, which gives a total score of 4.\n",
      "2. Applicant 2: The UGPA score is 3, and the ZFYA score is 0, which gives a total score of 3.\n",
      "3. Applicant 3: The UGPA score is 4, and the ZFYA score is 1, which gives a total score of 5.\n",
      "4. Applicant 4: The UGPA score is 4, and the ZFYA score is 0, which gives a total score of 4.\n",
      "5. Applicant 5: The UGPA score is 3, and the ZFYA score is -1, which gives a total score of 2.\n",
      "6. Applicant 6: The UGPA score is 4, and the ZFYA score is 0, which gives a total score of 4.\n",
      "\n",
      "Note: In this ranking system, a negative ZFYA score indicates a lower ranking than an applicant with a positive ZFYA score.\n"
     ]
    }
   ],
   "source": [
    "reponse = get_llama2_response(prompt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
